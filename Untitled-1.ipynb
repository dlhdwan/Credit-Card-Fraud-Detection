{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea610891",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection - Colab Version\n",
    "\n",
    "This notebook is adapted for Google Colab. It covers data preprocessing, handling imbalanced datasets (Undersampling & SMOTE), and benchmarking various machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c384e6f",
   "metadata": {},
   "source": [
    "## 1. Colab Environment Setup and Data Loading\n",
    "First, we import the necessary libraries. Since we are in Colab, we need a way to access the `creditcard.csv` file. You can either mount your Google Drive or upload the file directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54903496",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Option 1: Mount Google Drive (Uncomment if your file is in Drive)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# file_path = '/content/drive/MyDrive/path_to_your_file/creditcard.csv'\n",
    "\n",
    "# Option 2: Upload file directly\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Assuming the file is named 'creditcard.csv'\n",
    "import io\n",
    "# If you used Option 2, use this:\n",
    "df = pd.read_csv(io.BytesIO(uploaded['creditcard.csv']))\n",
    "\n",
    "# If you used Option 1, use this instead:\n",
    "# df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec2b02c",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis (EDA)\n",
    "Let's inspect the dataset structure, check for missing values, and visualize the class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc509ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first 5 rows\n",
    "display(df.head())\n",
    "\n",
    "# Check data shape and info\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce62e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum().max())\n",
    "\n",
    "# Class distribution\n",
    "print('\\nNo Frauds', round(df['Class'].value_counts()[0]/len(df) * 100,2), '% of the dataset')\n",
    "print('Frauds', round(df['Class'].value_counts()[1]/len(df) * 100,2), '% of the dataset')\n",
    "\n",
    "# Visualize Class Distribution\n",
    "sns.countplot(x='Class', data=df, palette='viridis')\n",
    "plt.title('Class Distribution \\n (0: No Fraud || 1: Fraud)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e107e87",
   "metadata": {},
   "source": [
    "## 3. Feature Scaling and Data Splitting\n",
    "We scale `Amount` and `Time` using `RobustScaler` because it is less prone to outliers. Then, we split the data using Stratified K-Fold to maintain the class ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d32ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Scale Amount and Time\n",
    "df['Amount_Scale'] = RobustScaler().fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['Time_Scale'] = RobustScaler().fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "# Drop original columns and insert scaled ones\n",
    "df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "df.insert(0, 'Amount_Scale', df['Amount_Scale'])\n",
    "df.insert(1, 'Time_Scale', df['Time_Scale'])\n",
    "\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43ad9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare X and y\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "# Stratified Split\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values\n",
    "\n",
    "print(\"Training set class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "print(dict(zip(unique, counts/len(y_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12affe69",
   "metadata": {},
   "source": [
    "## 4. Random Undersampling and Correlation Analysis\n",
    "To better understand correlations without the bias of the majority class, we create a balanced subsample (50/50 ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a917707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle data\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Create subsample\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "print(\"Distribution of the Classes in the subsample dataset\")\n",
    "print(new_df['Class'].value_counts()/len(new_df))\n",
    "\n",
    "sns.countplot(x='Class', data=new_df, palette='viridis')\n",
    "plt.title('Equally Distributed Classes', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d8b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Heatmap\n",
    "f, ax = plt.subplots(figsize=(24,20))\n",
    "\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax)\n",
    "plt.title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70699a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for Negative Correlation (V14, V12, V10) and Positive (V11, V4, V2, V19)\n",
    "f, axes = plt.subplots(ncols=4, figsize=(20,4))\n",
    "\n",
    "# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\n",
    "sns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=['blue', 'red'], ax=axes[0])\n",
    "axes[0].set_title('V14 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=['blue', 'red'], ax=axes[1])\n",
    "axes[1].set_title('V12 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=['blue', 'red'], ax=axes[2])\n",
    "axes[2].set_title('V10 vs Class Negative Correlation')\n",
    "\n",
    "sns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=['blue', 'red'], ax=axes[3])\n",
    "axes[3].set_title('V11 vs Class Positive Correlation')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4939535",
   "metadata": {},
   "source": [
    "## 5. Outlier Removal and Feature Transformation\n",
    "We remove extreme outliers from features with high correlation (V14, V12, V10) using the IQR method. We also apply `PowerTransformer` to fix skewness in features like V2 and V10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945b5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def outlier_cutoff(data, feature_name):\n",
    "    data_fraud = data[feature_name].loc[data['Class'] == 1].values\n",
    "    q1, q3 = np.percentile(data_fraud, 25), np.percentile(data_fraud, 75)\n",
    "    iqr = q3 - q1\n",
    "    cut_off = iqr * 1.5\n",
    "    lower, upper = q1 - cut_off, q3 + cut_off\n",
    "    \n",
    "    outlier_remove = data[(data['Class'] == 1) & ((data[feature_name] < lower) | (data[feature_name] > upper))]\n",
    "    print(f'Feature {feature_name} Outliers removed: {len(outlier_remove)}')\n",
    "    return data.drop(outlier_remove.index)\n",
    "\n",
    "# Remove outliers\n",
    "df_cleaned = new_df.copy()\n",
    "df_cleaned = outlier_cutoff(df_cleaned, 'V14')\n",
    "df_cleaned = outlier_cutoff(df_cleaned, 'V12')\n",
    "df_cleaned = outlier_cutoff(df_cleaned, 'V10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac67b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Apply PowerTransformer\n",
    "power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "feature_to_transform = ['V2', 'V10']\n",
    "\n",
    "df_cleaned[feature_to_transform] = power_transformer.fit_transform(df_cleaned[feature_to_transform])\n",
    "\n",
    "# Visualize distribution after transformation\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "sns.histplot(df_cleaned['V2'].loc[df_cleaned['Class'] == 1], ax=ax1, kde=True, color='#56F9BB')\n",
    "ax1.set_title('V2 Distribution (Transformed)')\n",
    "sns.histplot(df_cleaned['V10'].loc[df_cleaned['Class'] == 1], ax=ax2, kde=True, color='#FF5733')\n",
    "ax2.set_title('V10 Distribution (Transformed)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a36eb",
   "metadata": {},
   "source": [
    "## 6. Dimensionality Reduction Visualization\n",
    "Using t-SNE, PCA, and TruncatedSVD to visualize if the fraud and non-fraud classes are separable in 2D space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448c0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "X_red = df_cleaned.drop('Class', axis=1).values\n",
    "y_red = df_cleaned['Class'].values\n",
    "\n",
    "# t-SNE\n",
    "t0 = time.time()\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X_red)\n",
    "t1 = time.time()\n",
    "print(\"t-SNE took {:.2f} s\".format(t1 - t0))\n",
    "\n",
    "# PCA\n",
    "t0 = time.time()\n",
    "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X_red)\n",
    "t1 = time.time()\n",
    "print(\"PCA took {:.2f} s\".format(t1 - t0))\n",
    "\n",
    "# TruncatedSVD\n",
    "t0 = time.time()\n",
    "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X_red)\n",
    "t1 = time.time()\n",
    "print(\"Truncated SVD took {:.2f} s\".format(t1 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250ad295",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24, 6))\n",
    "f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n",
    "\n",
    "blue_patch = plt.Line2D([0], [0], marker='o', color='w', label='No Fraud', markerfacecolor='blue', markersize=10)\n",
    "red_patch = plt.Line2D([0], [0], marker='o', color='w', label='Fraud', markerfacecolor='red', markersize=10)\n",
    "\n",
    "# t-SNE scatter plot\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y_red == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y_red == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax1.set_title('t-SNE', fontsize=14)\n",
    "ax1.grid(True)\n",
    "ax1.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# PCA scatter plot\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_red == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y_red == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax2.set_title('PCA', fontsize=14)\n",
    "ax2.grid(True)\n",
    "ax2.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "# Truncated SVD scatter plot\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y_red == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y_red == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "ax3.set_title('Truncated SVD', fontsize=14)\n",
    "ax3.grid(True)\n",
    "ax3.legend(handles=[blue_patch, red_patch])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666cd589",
   "metadata": {},
   "source": [
    "## 7. SMOTE Oversampling Implementation\n",
    "We apply SMOTE to the training data to handle the imbalance. Note: We only apply SMOTE to the training set to avoid data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f777c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create copies of original data\n",
    "X_train_org = X_train.copy()\n",
    "y_train_org = y_train.copy()\n",
    "X_test_org = X_test.copy()\n",
    "y_test_org = y_test.copy()\n",
    "\n",
    "print('Length of X (train): {} | Length of y (train): {}'.format(len(X_train_org), len(y_train_org)))\n",
    "print('Length of X (test): {} | Length of y (test): {}'.format(len(X_test_org), len(y_test_org)))\n",
    "\n",
    "# Apply SMOTE\n",
    "sm = SMOTE(sampling_strategy='minority', random_state=42)\n",
    "X_train_smote, y_train_smote = sm.fit_resample(X_train_org, y_train_org)\n",
    "\n",
    "print('Length of X (train) after SMOTE: {}'.format(len(X_train_smote)))\n",
    "print('Length of y (train) after SMOTE: {}'.format(len(y_train_smote)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba92f638",
   "metadata": {},
   "source": [
    "## 8. Model Training and Evaluation\n",
    "We will train Logistic Regression, Random Forest, XGBoost, Decision Tree, and MLP on the SMOTE dataset and evaluate them on the original test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00118175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "def plot_model_performance(model, X_test, y_test, y_pred, model_name):\n",
    "    print(f\"--- {model_name} Report ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax[0], annot_kws={\"size\": 16})\n",
    "    ax[0].set_title(f'{model_name} - Confusion Matrix')\n",
    "    \n",
    "    # ROC Curve\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_prob = model.predict_proba(X_test)[:, 1]\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        ax[1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        ax[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        ax[1].set_title(f'{model_name} - ROC Curve')\n",
    "        ax[1].legend(loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32725988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Logistic Regression\n",
    "log_reg_sm = LogisticRegression(solver='liblinear', random_state=42)\n",
    "log_reg_sm.fit(X_train_smote, y_train_smote)\n",
    "y_pred_log = log_reg_sm.predict(X_test_org)\n",
    "plot_model_performance(log_reg_sm, X_test_org, y_test_org, y_pred_log, \"Logistic Regression (SMOTE)\")\n",
    "\n",
    "# 2. Random Forest\n",
    "rf_sm = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_sm.fit(X_train_smote, y_train_smote)\n",
    "y_pred_rf = rf_sm.predict(X_test_org)\n",
    "plot_model_performance(rf_sm, X_test_org, y_test_org, y_pred_rf, \"Random Forest (SMOTE)\")\n",
    "\n",
    "# 3. XGBoost\n",
    "xgb_sm = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "xgb_sm.fit(X_train_smote, y_train_smote)\n",
    "y_pred_xgb = xgb_sm.predict(X_test_org)\n",
    "plot_model_performance(xgb_sm, X_test_org, y_test_org, y_pred_xgb, \"XGBoost (SMOTE)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57cdc04",
   "metadata": {},
   "source": [
    "## 9. Hyperparameter Tuning for XGBoost\n",
    "Using `RandomizedSearchCV` to find the best parameters for XGBoost to further improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03622f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5, 1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_base = XGBClassifier(use_label_encoder=False, eval_metric='aucpr', random_state=42)\n",
    "\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_base,\n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=10, # Reduced iterations for faster execution in Colab demo\n",
    "    scoring='f1',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"Searching for best hyperparameters...\")\n",
    "xgb_random_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "best_xgb = xgb_random_search.best_estimator_\n",
    "print(f\"Best Parameters: {xgb_random_search.best_params_}\")\n",
    "\n",
    "y_pred_best_xgb = best_xgb.predict(X_test_org)\n",
    "plot_model_performance(best_xgb, X_test_org, y_test_org, y_pred_best_xgb, \"XGBoost (Tuned)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
